{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7004a127-a4dc-4522-95d8-0ec465915725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from itertools import count\n",
    "import sys\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d79a0e9f-1103-4d91-b14e-b8d54a717790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#expert_num = 64 # number of experts\n",
    "#expert_dim = 512\n",
    "#emb_dim = 256 # embedding size\n",
    "#token_num = 2048 # think token count, from unrolled batches possibly\n",
    "\n",
    "expert_num = 5 # number of experts\n",
    "expert_dim = 7\n",
    "emb_dim = 11 # embedding size\n",
    "token_num = 13 # think token count, from unrolled batches possibly\n",
    "\n",
    "# Bypass weight\n",
    "b = 0.1\n",
    "\n",
    "# Export sizes and bypass weight to C++ code as well\n",
    "exported = dict(\n",
    "    expert_count=expert_num,\n",
    "    expert_size=expert_dim,\n",
    "    embedding_size=emb_dim,\n",
    "    token_count=token_num,\n",
    "    b=np.array([b], dtype=np.float32) # I don't care that those numbers are longs, but I want this multiplier to be float32.\n",
    ")\n",
    "\n",
    "# token count * embedding size. Explicitly specifying order=C so it matches the C++ implementation.\n",
    "words = randn(token_num, emb_dim).astype(np.float32, order='C')\n",
    "exported[\"src\"] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ae5abfb-78af-4097-a3cd-891ee42a7133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experts_w1 = randn(expert_num, emb_dim, expert_dim).astype(np.float32, order='C')\n",
    "exported[\"experts_w1\"] = experts_w1\n",
    "\n",
    "experts_b1 = randn(expert_num, 1, expert_dim).astype(np.float32, order='C')\n",
    "exported[\"experts_b1\"] = experts_b1\n",
    "\n",
    "experts_w2 = randn(expert_num, expert_dim, emb_dim).astype(np.float32, order='C')\n",
    "exported[\"experts_w2\"] = experts_w2\n",
    "\n",
    "experts_b2 = randn(expert_num, 1, emb_dim).astype(np.float32, order='C')\n",
    "exported[\"experts_b2\"] = experts_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acd47b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scramble(a, axis=-1):\n",
    "    \"\"\"\n",
    "    Shuffle `a` in-place along the given axis.\n",
    "\n",
    "    Apply numpy.random.shuffle to the given axis of `a`.\n",
    "    Each one-dimensional slice is shuffled independently.\n",
    "    \"\"\"\n",
    "    b = a.swapaxes(axis, -1)\n",
    "    # Shuffle `b` in-place along the last axis.  `b` is a view of `a`,\n",
    "    # so `a` is shuffled in place, too.\n",
    "    shp = b.shape[:-1]\n",
    "    for ndx in np.ndindex(shp):\n",
    "        np.random.shuffle(b[ndx])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0525a698",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total is 52.0, expected 52\n"
     ]
    }
   ],
   "source": [
    "# Generate random binary router\n",
    "\n",
    "router = np.zeros((token_num, expert_num)).astype(np.float32)\n",
    "# Select K for TOP-K\n",
    "k = 4\n",
    "\n",
    "counter = 0\n",
    "expert = 0\n",
    "for t in range(token_num):\n",
    "    for counter in range(k):\n",
    "        router[t][expert + counter] = 1\n",
    "\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "#print(router)\n",
    "\n",
    "# router_shuf = np.transpose(router)\n",
    "scramble(router, axis=-1)\n",
    "# router = np.transpose(router_shuf)\n",
    "\n",
    "# Sanity check: sum per expert\n",
    "print(f'Total is {np.sum(router)}, expected {token_num * k}')\n",
    "# print(f'Expect around {token_num * k / expert_num} per expert')\n",
    "# print(np.sum(router, axis=0))\n",
    "\n",
    "# Turn the booleans into weights to fake a soft-max, normalised to 1.0 per token across experts\n",
    "for token_index in range(token_num):\n",
    "    expert_indices, = router[token_index].nonzero()\n",
    "    w = np.abs(randn(len(expert_indices)))\n",
    "    router[token_index][expert_indices] = w / np.sum(w)\n",
    "\n",
    "# Sanity check: weights per token sum to 1.0\n",
    "# print(np.sum(router, axis=1))\n",
    "\n",
    "# Order is in [token,expert]. Flip it back to [expert,token] to make the for-loop easier\n",
    "router = np.transpose(router)\n",
    "\n",
    "# Order='C' necessary otherwise it will be saved flipped, since we transposed router\n",
    "# and numpy implements this by just changing the memory order marker.\n",
    "exported[\"router\"] = router.astype(np.float32, order='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70429174-2e92-44b1-8bf0-e4aed9a6a890",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 0:\n",
      "Expert 0 gets 10 tokens\n",
      "  (10, 11) * (11, 7) + (1, 7) = (10, 7)\n",
      "\n",
      "  (10, 7) * (7, 11) + (1, 11) = (10, 11)\n",
      "\n",
      "Expert 1:\n",
      "Expert 1 gets 10 tokens\n",
      "  (10, 11) * (11, 7) + (1, 7) = (10, 7)\n",
      "\n",
      "  (10, 7) * (7, 11) + (1, 11) = (10, 11)\n",
      "\n",
      "Expert 2:\n",
      "Expert 2 gets 9 tokens\n",
      "  (9, 11) * (11, 7) + (1, 7) = (9, 7)\n",
      "\n",
      "  (9, 7) * (7, 11) + (1, 11) = (9, 11)\n",
      "\n",
      "Expert 3:\n",
      "Expert 3 gets 12 tokens\n",
      "  (12, 11) * (11, 7) + (1, 7) = (12, 7)\n",
      "\n",
      "  (12, 7) * (7, 11) + (1, 11) = (12, 11)\n",
      "\n",
      "Expert 4:\n",
      "Expert 4 gets 11 tokens\n",
      "  (11, 11) * (11, 7) + (1, 7) = (11, 7)\n",
      "\n",
      "  (11, 7) * (7, 11) + (1, 11) = (11, 11)\n",
      "\n",
      "Total output: (13, 11)\n"
     ]
    }
   ],
   "source": [
    "# words(7) * emb_size(4)\n",
    "total_output = b * words # … so that if a word doesn't go through any expert it will still have some value\n",
    "\n",
    "# Repeat for every expert, looking at routed do determine which words go to said expert\n",
    "for n, expert_w1, expert_b1, expert_w2, expert_b2, mask in zip(count(), experts_w1, experts_b1, experts_w2, experts_b2, router):\n",
    "    print(f\"Expert {n}:\")\n",
    "    print(f\"Expert {n} gets {mask.nonzero()[0].shape[0]} tokens\")\n",
    "    assert mask.shape[0] == words.shape[0]\n",
    "    \n",
    "    # select all words where the mask for this expert is > 0\n",
    "    expert_input = words[mask.nonzero()]\n",
    "    \n",
    "    # These mask > 0 values also function as the weights for the output\n",
    "    expert_output_weights = mask[mask.nonzero()]\n",
    "    \n",
    "    # classic matmul op for feed-forward I guess? + relu\n",
    "    expert_output1 = np.maximum(0.0, np.add(np.matmul(expert_input, expert_w1), expert_b1))\n",
    "    expert_output2 = np.maximum(0.0, np.add(np.matmul(expert_output1, expert_w2), expert_b2))\n",
    "    assert expert_output2.shape[0] == mask[mask > 0].shape[0]\n",
    "    \n",
    "    # Add all the selected experts' outputs to the final outputs. Weigh them by the router.\n",
    "    # (newaxis trick used to broadcast the 1d [token_count] array to the 2d [token_count, emb_dim] of expert_output2)\n",
    "    total_output[mask.nonzero()] += expert_output_weights[:,np.newaxis] * expert_output2\n",
    "\n",
    "    print(f\"  {expert_input.shape} * {expert_w1.shape} + {expert_b1.shape} = {expert_output1.shape}\\n\")\n",
    "    print(f\"  {expert_output1.shape} * {expert_w2.shape} + {expert_b2.shape} = {expert_output2.shape}\\n\")\n",
    "    \n",
    "    exported[f\"expert_{n}_src\"] = expert_input.astype(np.float32, order='C')\n",
    "    exported[f\"expert_{n}_dst\"] = expert_output2.astype(np.float32, order='C')\n",
    "    \n",
    "print(f\"Total output: {total_output.shape}\")\n",
    "exported[\"dst\"] = total_output.astype(np.float32, order='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "261b8f50-16b9-456d-be41-43a568d3b4eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.savez(\"data.npz\", **exported)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b06da5d3-e159-4ea0-bdf7-5f6c81d38e5f",
   "metadata": {},
   "source": [
    "# Do grid search generation for various variable values. Save them to separate files.\n",
    "# Do not run automatically LOL\n",
    "\n",
    "expert_num_list = [8, 16, 32, 64, 128, 256] # number of experts\n",
    "expert_dim_list = [256, 512, 1024, 2048, 4096]\n",
    "emb_dim_list = [256, 512, 1024, 2048, 4096] # embedding size\n",
    "token_num_list = [128, 256, 512, 1024, 2048, 4096, 8192] # think token count, from unrolled batches possibly\n",
    "\n",
    "\n",
    "for expert_num in expert_num_list:\n",
    "    for expert_dim in expert_dim_list:\n",
    "        for emb_dim in emb_dim_list:\n",
    "            for token_num in token_num_list:\n",
    "                \n",
    "                exported = dict()\n",
    "                \n",
    "                # Generate input\n",
    "                words = randn(token_num, emb_dim).astype(np.float32)\n",
    "                exported[\"src\"] = words\n",
    "                \n",
    "                # Generate parameters\n",
    "                experts_w1 = randn(expert_num, emb_dim, expert_dim).astype(np.float32)\n",
    "                exported[\"experts_w1\"] = experts_w1\n",
    "\n",
    "                # expert (3) * emb_prime_size (4) * emb_size (5)\n",
    "                experts_b1 = randn(expert_num, 1, expert_dim).astype(np.float32)\n",
    "                exported[\"experts_b1\"] = experts_b1\n",
    "\n",
    "                # expert (3) * emb_size (5) * emb_size (5)\n",
    "                experts_w2 = randn(expert_num, expert_dim, emb_dim).astype(np.float32)\n",
    "                exported[\"experts_w2\"] = experts_w2\n",
    "\n",
    "                # expert (3) * emb_prime_size (4) * emb_size (5)\n",
    "                experts_b2 = randn(expert_num, 1, emb_dim).astype(np.float32)\n",
    "                exported[\"experts_b2\"] = experts_b2\n",
    "\n",
    "                b = 0.1\n",
    "\n",
    "                # words(7) * emb_size(4)\n",
    "                total_output = b * words # … so that if a word doesn't go through any expert it will still have some value\n",
    "\n",
    "                # Repeat for every expert, looking at routed do determine which words go to said expert\n",
    "                for n, expert_w1, expert_b1, expert_w2, expert_b2, mask in zip(count(), experts_w1, experts_b1, experts_w2, experts_b2, router):\n",
    "                    #print(f\"Expert {n}:\")\n",
    "\n",
    "                    # select all words where the mask for this expert is > 0\n",
    "                    expert_input = words[mask.nonzero()]\n",
    "\n",
    "                    # classic matmul op for feed-forward I guess? + relu\n",
    "                    expert_output1 = np.maximum(0.0, np.add(np.matmul(expert_input, expert_w1), expert_b1))\n",
    "                    expert_output2 = np.maximum(0.0, np.add(np.matmul(expert_output1, expert_w2), expert_b2))\n",
    "\n",
    "                    # I did assignment here, but could also be addition\n",
    "                    total_output[mask.nonzero()] = expert_output2\n",
    "\n",
    "                    #print(f\"  {expert_input.shape} * {expert_w1.shape} + {expert_b1.shape} = {expert_output1.shape}\\n\")\n",
    "                    #print(f\"  {expert_output1.shape} * {expert_w2.shape} + {expert_b2.shape} = {expert_output2.shape}\\n\")\n",
    "\n",
    "                    exported[f\"expert_{n}_src\"] = expert_input\n",
    "                    exported[f\"expert_{n}_dst\"] = expert_output2\n",
    "\n",
    "                #print(f\"Total output: {total_output.shape}\")\n",
    "                exported[\"dst\"] = total_output\n",
    "                \n",
    "                file_name = \"exp-num_\" + str(expert_num) + \"_exp-dim_\" + str(expert_dim) + \"_emb-dim_\" + str(emb_dim) + \"_token-num_\" + str(token_num)\n",
    "                np.savez_compressed(file_name + \".npz\", **exported)\n",
    "                print(\"Finished \", file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffb31c3-0748-4002-815f-925ae1fb0267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e150e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
